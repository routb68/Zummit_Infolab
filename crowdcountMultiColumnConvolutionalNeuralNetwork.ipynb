{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM25ibDkGhb/B/Nid9A2gR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/routb68/Zummit_Infolab/blob/main/crowdcountMultiColumnConvolutionalNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing library "
      ],
      "metadata": {
        "id": "J5gTxtShrFMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "DwwqFFzGqn4h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netwok creation"
      ],
      "metadata": {
        "id": "uncc0drignYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "imc3UxNJfDgA"
      },
      "outputs": [],
      "source": [
        "class Conv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, NL='relu', same_padding=False, bn=False):\n",
        "        super(Conv2d, self).__init__()\n",
        "        padding = int((kernel_size - 1) / 2) if same_padding else 0\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0, affine=True) if bn else None\n",
        "        if NL == 'relu' :\n",
        "            self.relu = nn.ReLU(inplace=True) \n",
        "        elif NL == 'prelu':\n",
        "            self.relu = nn.PReLU() \n",
        "        else:\n",
        "            self.relu = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, in_features, out_features, NL='relu'):\n",
        "        super(FC, self).__init__()\n",
        "        self.fc = nn.Linear(in_features, out_features)\n",
        "        if NL == 'relu' :\n",
        "            self.relu = nn.ReLU(inplace=True) \n",
        "        elif NL == 'prelu':\n",
        "            self.relu = nn.PReLU() \n",
        "        else:\n",
        "            self.relu = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def save_net(fname, net):\n",
        "    import h5py\n",
        "    h5f = h5py.File(fname, mode='w')\n",
        "    for k, v in net.state_dict().items():\n",
        "        h5f.create_dataset(k, data=v.cpu().numpy())\n",
        "\n",
        "\n",
        "def load_net(fname, net):\n",
        "    import h5py\n",
        "    h5f = h5py.File(fname, mode='r')\n",
        "    for k, v in net.state_dict().items():        \n",
        "        param = torch.from_numpy(np.asarray(h5f[k])) \n",
        "        v.copy_(param)\n",
        "\n",
        "\n",
        "\n",
        "def np_to_variable(x, is_cuda=True, is_training=False, dtype=torch.FloatTensor):\n",
        "    if is_training:\n",
        "        v = Variable(torch.from_numpy(x).type(dtype))\n",
        "    else:\n",
        "        v = Variable(torch.from_numpy(x).type(dtype), requires_grad = False, volatile = True)\n",
        "    if is_cuda:\n",
        "        v = v.cuda()\n",
        "    return v\n",
        "\n",
        "\n",
        "def set_trainable(model, requires_grad):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = requires_grad\n",
        "\n",
        "\n",
        "def weights_normal_init(model, dev=0.01):\n",
        "    if isinstance(model, list):\n",
        "        for m in model:\n",
        "            weights_normal_init(m, dev)\n",
        "    else:\n",
        "        for m in model.modules():            \n",
        "            if isinstance(m, nn.Conv2d):        \n",
        "                m.weight.data.normal_(0.0, dev)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(0.0)\n",
        "\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0.0, dev)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "yIIfHuV4hJ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CMTL(nn.Module):\n",
        "    '''\n",
        "    Implementation of CNN-based Cascaded Multi-task Learning of High-level Prior and Density\n",
        "    Estimation for Crowd Counting (Sindagi et al.)\n",
        "    '''\n",
        "    def __init__(self, bn=False, num_classes=10):\n",
        "        super(CMTL, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes        \n",
        "        self.base_layer = nn.Sequential(Conv2d( 1, 16, 9, same_padding=True, NL='prelu', bn=bn),                                     \n",
        "                                        Conv2d(16, 32, 7, same_padding=True, NL='prelu', bn=bn))\n",
        "        \n",
        "        self.hl_prior_1 = nn.Sequential(Conv2d( 32, 16, 9, same_padding=True, NL='prelu', bn=bn),\n",
        "                                     nn.MaxPool2d(2),\n",
        "                                     Conv2d(16, 32, 7, same_padding=True, NL='prelu', bn=bn),\n",
        "                                     nn.MaxPool2d(2),\n",
        "                                     Conv2d(32, 16, 7, same_padding=True, NL='prelu', bn=bn),\n",
        "                                     Conv2d(16, 8,  7, same_padding=True, NL='prelu', bn=bn))\n",
        "                \n",
        "        self.hl_prior_2 = nn.Sequential(nn.AdaptiveMaxPool2d((32,32)),\n",
        "                                        Conv2d( 8, 4, 1, same_padding=True, NL='prelu', bn=bn))\n",
        "        \n",
        "        self.hl_prior_fc1 = FC(4*1024,512, NL='prelu')\n",
        "        self.hl_prior_fc2 = FC(512,256,    NL='prelu')\n",
        "        self.hl_prior_fc3 = FC(256, self.num_classes,     NL='prelu')\n",
        "        \n",
        "        \n",
        "        self.de_stage_1 = nn.Sequential(Conv2d( 32, 20, 7, same_padding=True, NL='prelu', bn=bn),\n",
        "                                     nn.MaxPool2d(2),\n",
        "                                     Conv2d(20, 40, 5, same_padding=True, NL='prelu', bn=bn),\n",
        "                                     nn.MaxPool2d(2),\n",
        "                                     Conv2d(40, 20, 5, same_padding=True, NL='prelu', bn=bn),\n",
        "                                     Conv2d(20, 10, 5, same_padding=True, NL='prelu', bn=bn))\n",
        "        \n",
        "        self.de_stage_2 = nn.Sequential(Conv2d( 18, 24, 3, same_padding=True, NL='prelu', bn=bn),\n",
        "                                        Conv2d( 24, 32, 3, same_padding=True, NL='prelu', bn=bn),                                        \n",
        "                                        nn.ConvTranspose2d(32,16,4,stride=2,padding=1,output_padding=0,bias=True),\n",
        "                                        nn.PReLU(),\n",
        "                                        nn.ConvTranspose2d(16,8,4,stride=2,padding=1,output_padding=0,bias=True),\n",
        "                                        nn.PReLU(),\n",
        "                                        Conv2d(8, 1, 1, same_padding=True, NL='relu', bn=bn))\n",
        "        \n",
        "    def forward(self, im_data):\n",
        "        x_base = self.base_layer(im_data)\n",
        "        x_hlp1 = self.hl_prior_1(x_base)\n",
        "        x_hlp2 = self.hl_prior_2(x_hlp1)\n",
        "        x_hlp2 = x_hlp2.view(x_hlp2.size()[0], -1) \n",
        "        x_hlp = self.hl_prior_fc1(x_hlp2)\n",
        "        x_hlp = F.dropout(x_hlp, training=self.training)\n",
        "        x_hlp = self.hl_prior_fc2(x_hlp)\n",
        "        x_hlp = F.dropout(x_hlp, training=self.training)\n",
        "        x_cls = self.hl_prior_fc3(x_hlp)        \n",
        "        x_den = self.de_stage_1(x_base)        \n",
        "        x_den = torch.cat((x_hlp1,x_den),1)\n",
        "        x_den = self.de_stage_2(x_den)\n",
        "        return x_den, x_cls"
      ],
      "metadata": {
        "id": "ChT4lkgzhLlt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "crowd count module"
      ],
      "metadata": {
        "id": "9IQC1KDEglts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrowdCounter(nn.Module):\n",
        "    def __init__(self, ce_weights=None):\n",
        "        super(CrowdCounter, self).__init__()        \n",
        "        self.CCN = CMTL()\n",
        "        if ce_weights is not None:\n",
        "            ce_weights = torch.Tensor(ce_weights)\n",
        "            ce_weights = ce_weights.cuda()\n",
        "        self.loss_mse_fn = nn.MSELoss()\n",
        "        self.loss_bce_fn = nn.BCELoss(weight=ce_weights)\n",
        "        \n",
        "    @property\n",
        "    def loss(self):\n",
        "        return self.loss_mse + 0.0001*self.cross_entropy\n",
        "    \n",
        "    def forward(self,  im_data, gt_data=None, gt_cls_label=None, ce_weights=None):        \n",
        "        im_data = np_to_variable(im_data, is_cuda=True, is_training=self.training)                        \n",
        "        density_map, density_cls_score = self.CCN(im_data)\n",
        "        density_cls_prob = F.softmax(density_cls_score)\n",
        "        \n",
        "        if self.training:                        \n",
        "            gt_data = np_to_variable(gt_data, is_cuda=True, is_training=self.training)            \n",
        "            gt_cls_label = np_to_variable(gt_cls_label, is_cuda=True, is_training=self.training,dtype=torch.FloatTensor)                        \n",
        "            self.loss_mse, self.cross_entropy = self.build_loss(density_map, density_cls_prob, gt_data, gt_cls_label, ce_weights)\n",
        "            \n",
        "            \n",
        "        return density_map\n",
        "    \n",
        "    def build_loss(self, density_map, density_cls_score, gt_data, gt_cls_label, ce_weights):\n",
        "        loss_mse = self.loss_mse_fn(density_map, gt_data)        \n",
        "        ce_weights = torch.Tensor(ce_weights)\n",
        "        ce_weights = ce_weights.cuda()\n",
        "        cross_entropy = self.loss_bce_fn(density_cls_score, gt_cls_label)\n",
        "        return loss_mse, cross_entropy\n"
      ],
      "metadata": {
        "id": "tpIA9fKqhDbK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading"
      ],
      "metadata": {
        "id": "MtyyTMRWnJv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataLoader():\n",
        "    def __init__(self, data_path, gt_path, shuffle=False, gt_downsample=False, pre_load=False, num_classes=10):\n",
        "        #pre_load: if true, all training and validation images are loaded into CPU RAM for faster processing.\n",
        "        #          This avoids frequent file reads. Use this only for small datasets.\n",
        "        #num_classes: total number of classes into which the crowd count is divided (default: 10 as used in the paper)\n",
        "        self.data_path = data_path\n",
        "        self.gt_path = gt_path\n",
        "        self.gt_downsample = gt_downsample\n",
        "        self.pre_load = pre_load\n",
        "        self.data_files = [filename for filename in os.listdir(data_path) \\\n",
        "                           if os.path.isfile(os.path.join(data_path,filename))]\n",
        "        self.data_files.sort()\n",
        "        self.shuffle = shuffle\n",
        "        if shuffle:\n",
        "            random.seed(2468)\n",
        "        self.num_samples = len(self.data_files)\n",
        "        self.blob_list = {}        \n",
        "        self.id_list = range(0,self.num_samples)\n",
        "        self.min_gt_count = sys.maxint\n",
        "        self.max_gt_count = 0\n",
        "        self.num_classes = num_classes\n",
        "        self.count_class_hist = np.zeros(self.num_classes)        \n",
        "        if self.pre_load:\n",
        "            self.preload_data() #load input images and grount truth into memory                \n",
        "            self.assign_gt_class_labels() #assign ground truth crowd group/class labels to each image\n",
        "            \n",
        "        else:\n",
        "            self.get_stats_in_dataset() #get min - max crowd count present in the dataset. used later for assigning crowd group/class\n",
        "            \n",
        "    \n",
        "    def get_classifier_weights(self):\n",
        "        #since the dataset is imbalanced, classifier weights are used to ensure balance.\n",
        "        #this function returns weights for each class based on the number of samples available for each class\n",
        "        wts = self.count_class_hist\n",
        "        wts = 1-wts/(sum(wts));\n",
        "        wts = wts/sum(wts);\n",
        "        return wts\n",
        "        \n",
        "    def preload_data(self):\n",
        "        print ('Pre-loading the data. This may take a while...')\n",
        "        idx = 0\n",
        "        for fname in self.data_files:            \n",
        "            img, den, gt_count = self.read_image_and_gt(fname)\n",
        "            self.min_gt_count = min(self.min_gt_count, gt_count)\n",
        "            self.max_gt_count = max(self.max_gt_count, gt_count)\n",
        "            \n",
        "            blob = {}\n",
        "            blob['data']=img\n",
        "            blob['gt_density']=den\n",
        "            blob['fname'] = fname                                \n",
        "            blob['gt_count'] = gt_count\n",
        "            \n",
        "            self.blob_list[idx] = blob\n",
        "            idx = idx+1\n",
        "            if idx % 100 == 0:                               \n",
        "                print ('Loaded ', idx , '/' , self.num_samples)\n",
        "        print ('Completed laoding ' ,idx, 'files')\n",
        "        \n",
        "        \n",
        "    def assign_gt_class_labels(self):        \n",
        "        for i in range(0,self.num_samples):\n",
        "            gt_class_label = np.zeros(self.num_classes, dtype=np.int)\n",
        "            bin_val = (self.max_gt_count - self.min_gt_count)/float(self.num_classes)\n",
        "            class_idx = np.round(self.blob_list[i]['gt_count']/bin_val)\n",
        "            class_idx = int(min(class_idx,self.num_classes-1))\n",
        "            gt_class_label[class_idx]=1\n",
        "            self.blob_list[i]['gt_class_label'] = gt_class_label.reshape(1,gt_class_label.shape[0])\n",
        "            self.count_class_hist[class_idx] += 1\n",
        "            \n",
        "                    \n",
        "    def __iter__(self):\n",
        "        if self.shuffle:            \n",
        "            if self.pre_load:            \n",
        "                random.shuffle(self.id_list)        \n",
        "            else:\n",
        "                random.shuffle(self.data_files)\n",
        "                \n",
        "        files = self.data_files\n",
        "        id_list = self.id_list\n",
        "       \n",
        "        \n",
        "        for idx in id_list:\n",
        "            if self.pre_load:\n",
        "                blob = self.blob_list[idx]    \n",
        "                blob['idx'] = idx\n",
        "            else:\n",
        "                                    \n",
        "                fname = files[idx]\n",
        "                img, den, gt_count = self.read_image_and_gt(fname)\n",
        "                gt_class_label = np.zeros(self.num_classes,dtype=np.int)\n",
        "                bin_val = (self.max_gt_count - self.min_gt_count)/float(self.num_classes)\n",
        "                class_idx = np.round(gt_count/bin_val)\n",
        "                class_idx = int(min(class_idx,self.num_classes-1) )             \n",
        "                gt_class_label[class_idx] = 1\n",
        "                \n",
        "                blob = {}\n",
        "                blob['data']=img\n",
        "                blob['gt_density']=den\n",
        "                blob['fname'] = fname\n",
        "                blob['gt_count'] = gt_count\n",
        "                blob['gt_class_label'] = gt_class_label.reshape(1,gt_class_label.shape[0])\n",
        "                \n",
        "                \n",
        "                \n",
        "            yield blob\n",
        "    \n",
        "    def get_stats_in_dataset(self):\n",
        "        \n",
        "        min_count = sys.maxint\n",
        "        max_count = 0\n",
        "        gt_count_array = np.zeros(self.num_samples)\n",
        "        i = 0\n",
        "        for fname in self.data_files:\n",
        "            den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).as_matrix()                        \n",
        "            den  = den.astype(np.float32, copy=False)\n",
        "            gt_count = np.sum(den)\n",
        "            min_count = min(min_count, gt_count)\n",
        "            max_count = max(max_count, gt_count)\n",
        "            gt_count_array[i] = gt_count\n",
        "            i+=1\n",
        "        \n",
        "        self.min_gt_count = min_count\n",
        "        self.max_gt_count = max_count        \n",
        "        bin_val = (self.max_gt_count - self.min_gt_count)/float(self.num_classes)\n",
        "        class_idx_array = np.round(gt_count_array/bin_val)\n",
        "        \n",
        "        for class_idx in class_idx_array:\n",
        "            class_idx = int(min(class_idx, self.num_classes-1))\n",
        "            self.count_class_hist[class_idx]+=1\n",
        "            \n",
        "        \n",
        "    def get_num_samples(self):\n",
        "        return self.num_samples\n",
        "                \n",
        "    def read_image_and_gt(self,fname):\n",
        "        img = cv2.imread(os.path.join(self.data_path,fname),0)\n",
        "        img = img.astype(np.float32, copy=False)\n",
        "        ht = img.shape[0]\n",
        "        wd = img.shape[1]\n",
        "        ht_1 = (ht/4)*4\n",
        "        wd_1 = (wd/4)*4\n",
        "        img = cv2.resize(img,(wd_1,ht_1))\n",
        "        img = img.reshape((1,1,img.shape[0],img.shape[1]))\n",
        "        den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).as_matrix()                        \n",
        "        den  = den.astype(np.float32, copy=False)\n",
        "        if self.gt_downsample:\n",
        "            wd_1 = wd_1/4\n",
        "            ht_1 = ht_1/4\n",
        "            den = cv2.resize(den,(wd_1,ht_1))                \n",
        "            den = den * ((wd*ht)/(wd_1*ht_1))\n",
        "        else:\n",
        "            den = cv2.resize(den,(wd_1,ht_1))\n",
        "            den = den * ((wd*ht)/(wd_1*ht_1))\n",
        "            \n",
        "        den = den.reshape((1,1,den.shape[0],den.shape[1]))  \n",
        "        gt_count = np.sum(den)\n",
        "        \n",
        "        return img, den, gt_count\n",
        "        \n",
        "            \n",
        "        "
      ],
      "metadata": {
        "id": "NJWPFJxGnJDe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Timer"
      ],
      "metadata": {
        "id": "ibVFVMjWpOMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class Timer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tot_time = 0.\n",
        "        self.calls = 0\n",
        "        self.start_time = 0.\n",
        "        self.diff = 0.\n",
        "        self.average_time = 0.\n",
        "\n",
        "    def tic(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def toc(self, average=True):\n",
        "        self.diff = time.time() - self.start_time\n",
        "        self.tot_time  += self.diff\n",
        "        self.calls += 1\n",
        "        self.average_time = self.tot_time / self.calls\n",
        "        if average:\n",
        "            return self.average_time\n",
        "        else:\n",
        "            return self.diff"
      ],
      "metadata": {
        "id": "Mt6sBCGipP1x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of model"
      ],
      "metadata": {
        "id": "oZcBtI1dpVq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(trained_model, data_loader):    \n",
        "    net = CrowdCounter()\n",
        "    load_net(trained_model, net)\n",
        "    net.cuda()\n",
        "    net.eval()\n",
        "    mae = 0.0\n",
        "    mse = 0.0\n",
        "    for blob in data_loader:                        \n",
        "        im_data = blob['data']\n",
        "        gt_data = blob['gt_density']\n",
        "        density_map = net(im_data, gt_data)\n",
        "        density_map = density_map.data.cpu().numpy()\n",
        "        gt_count = np.sum(gt_data)\n",
        "        et_count = np.sum(density_map)\n",
        "        mae += abs(gt_count-et_count)\n",
        "        mse += ((gt_count-et_count)*(gt_count-et_count))        \n",
        "    mae = mae/data_loader.get_num_samples()\n",
        "    mse = np.sqrt(mse/data_loader.get_num_samples())\n",
        "    return mae,mse"
      ],
      "metadata": {
        "id": "FbEYVX_1pYW2"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}