{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/routb68/Zummit_Infolab/blob/main/crowd_26Dec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ut99e5pJXCC",
        "outputId": "bc3824c2-8a89-4de9-c31a-6c35c2633fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tJtYHdw4Kbfj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import pdb\n",
        "import numpy as np\n",
        "\n",
        "__all__ = ['Inception3', 'inception_v3']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    # Inception v3 ported from TensorFlow\n",
        "    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def headCount_inceptionv3(pretrained=False, **kwargs):\n",
        "    r\"\"\"Inception v3 model architecture from\n",
        "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        if 'transform_input' not in kwargs:\n",
        "            kwargs['transform_input'] = True\n",
        "        model = Inception3(**kwargs)\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['inception_v3_google']),strict=False)\n",
        "        return model\n",
        "\n",
        "    return Inception3(**kwargs)\n",
        "\n",
        "\n",
        "class Inception3(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1000, aux_logits=False, transform_input=False):\n",
        "        super(Inception3, self).__init__()\n",
        "        self.aux_logits = aux_logits\n",
        "        self.transform_input = transform_input\n",
        "        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
        "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3, padding=1)\n",
        "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
        "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
        "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
        "        self.Mixed_6a = InceptionB(288)\n",
        "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
        "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
        "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
        "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
        "        if aux_logits:\n",
        "            self.AuxLogits = InceptionAux(768, num_classes)\n",
        "        self.Mixed_7a = InceptionD(768)\n",
        "        self.Mixed_7b = InceptionE(1280)\n",
        "        self.Mixed_7c = InceptionE(2048)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.lconv1 = nn.Conv2d(288, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
        "        self.lconv2 = nn.Conv2d(768, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
        "        self.lconv3 = nn.Conv2d(2048, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
        "        self.att_conv = nn.Conv2d(2048, 1, kernel_size = 1, stride=1, padding=0, bias=False)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                import scipy.stats as stats\n",
        "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
        "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
        "                values = torch.Tensor(X.rvs(m.weight.numel()))\n",
        "                values = values.view(m.weight.size())\n",
        "                m.weight.data.copy_(values)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.transform_input:\n",
        "            x = x.clone()\n",
        "            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "        # 299 x 299 x 3\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # 149 x 149 x 32\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # 147 x 147 x 32\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # 147 x 147 x 64\n",
        "        # x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "        # 73 x 73 x 64\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # 73 x 73 x 80\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # 71 x 71 x 192\n",
        "        # x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "        # 35 x 35 x 192\n",
        "        x = self.Mixed_5b(x)\n",
        "        # 35 x 35 x 256\n",
        "        x = self.Mixed_5c(x)\n",
        "        # 35 x 35 x 288\n",
        "        x = self.Mixed_5d(x)\n",
        "        # 35 x 35 x 288\n",
        "        # 128x128x288\n",
        "        x = self.Mixed_6a(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6b(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6c(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6d(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_6e(x)\n",
        "        # 64x64x768\n",
        "        # 17 x 17 x 768\n",
        "                \n",
        "        if self.training and self.aux_logits:\n",
        "            aux = self.AuxLogits(x)\n",
        "        # 17 x 17 x 768\n",
        "        x = self.Mixed_7a(x)\n",
        "        # 8 x 8 x 1280\n",
        "        x = self.Mixed_7b(x)\n",
        "        # 8 x 8 x 2048\n",
        "        x = self.upsample(x)\n",
        "        attention_map = self.sigm(self.att_conv(x))\n",
        "        feature_map3 = self.Mixed_7c(x)\n",
        "        feature_map3 = feature_map3*attention_map\n",
        "        # 32x32x2048\n",
        "        #feature_map4 = F.avg_pool2d(feature_map3,2)\n",
        "        # x_cat = feature_map3\n",
        "        #density_map1 = self.lconv1(feature_map1)\n",
        "        #density_map1 = density_map1.view(-1,density_map1.size(2),density_map1.size(3))\n",
        "        #density_map2 = self.lconv2(feature_map2)\n",
        "        #density_map2 = density_map2.view(-1,density_map2.size(2),density_map2.size(3))\n",
        "        density_map3 = self.lconv3(feature_map3)\n",
        "        density_map3 = self.relu(density_map3)\n",
        "        density_map3 = density_map3.view(-1,density_map3.size(2),density_map3.size(3))\n",
        "        attention_map = attention_map.view(-1,attention_map.size(2),attention_map.size(3))\n",
        "        \n",
        "        #density_map4 = self.lconv4(feature_map4)\n",
        "        #density_map4 = density_map4.view(-1,density_map4.size(2),density_map4.size(3))\n",
        "        # density_map = F.avg_pool2d(density_map,kernel_size=2)\n",
        "        return density_map3,attention_map\n",
        "\n",
        "class SequenceWise(nn.Module):\n",
        "    def __init__(self, module):\n",
        "        \"\"\"\n",
        "        Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
        "        Allows handling of variable sequence lengths and minibatch sizes.\n",
        "        :param module: Module to apply input to.\n",
        "        \"\"\"\n",
        "        super(SequenceWise, self).__init__()\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self, x):\n",
        "        t, n = x.size(0), x.size(1)\n",
        "        x = x.contiguous().view(t * n, -1)\n",
        "        x = self.module(x)\n",
        "        x = x.view(t, n, -1)\n",
        "        x = x.permute(1,0,2)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        tmpstr = self.__class__.__name__ + ' (\\n'\n",
        "        tmpstr += self.module.__repr__()\n",
        "        tmpstr += ')'\n",
        "        return tmpstr\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(InceptionA, self).__init__()\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
        "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionB, self).__init__()\n",
        "        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(InceptionC, self).__init__()\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
        "\n",
        "        c7 = channels_7x7\n",
        "        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "\n",
        "        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "\n",
        "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionD, self).__init__()\n",
        "        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
        "        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
        "        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = self.branch3x3_2(branch3x3)\n",
        "\n",
        "        branch7x7x3 = self.branch7x7x3_1(x)\n",
        "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
        "\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionE(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionE, self).__init__()\n",
        "        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n",
        "\n",
        "        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n",
        "        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(InceptionAux, self).__init__()\n",
        "        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n",
        "        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n",
        "        self.conv1.stddev = 0.01\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "        self.fc.stddev = 0.001\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 17 x 17 x 768\n",
        "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
        "        # 5 x 5 x 768\n",
        "        x = self.conv0(x)\n",
        "        # 5 x 5 x 128\n",
        "        x = self.conv1(x)\n",
        "        # 1 x 1 x 768\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 768\n",
        "        x = self.fc(x)\n",
        "        # 1000\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1LfysOIWK251"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.spatial\n",
        "import pdb\n",
        "def generate_multi_density_map(shape=(5,5),points=None,f_sz=15,sigma=4,num=3):\n",
        "    '''\n",
        "    generate multiple density maps according to the density\n",
        "    '''\n",
        "    # calculate the distance of each point to the nearest neighbour\n",
        "    dist = scipy.spatial.distance.cdist(points,points,metric='euclidean')\n",
        "    dist.sort()\n",
        "    k = 3\n",
        "    f_sz_vec = [15,15,15]\n",
        "    meanDist = dist[:,1:k+1].mean(axis=1)\n",
        "    thresholds = np.array([[0,20],[20,50],[50,1e9]])\n",
        "    density_map = np.zeros((num,shape[0],shape[1]))\n",
        "    for i in range(num):\n",
        "        selector = (meanDist>thresholds[i,0]) & (meanDist<=thresholds[i,1])\n",
        "        points_subset = points[selector,:]\n",
        "        density_map[i,] = generate_density_map(shape,points_subset,f_sz_vec[i],sigma)\n",
        "    return density_map\n",
        "        \n",
        "def generate_density_map(shape=(5,5),points=None,f_sz=15,sigma=4):\n",
        "    \"\"\"\n",
        "    generate density map given head coordinations\n",
        "    \"\"\"\n",
        "    im_density = np.zeros(shape[0:2])\n",
        "    h, w = shape[0:2]\n",
        "    if len(points) == 0:\n",
        "        return im_density\n",
        "    for j in range(len(points)):\n",
        "        H = matlab_style_gauss2D((f_sz,f_sz),sigma)\n",
        "        x = np.minimum(w,np.maximum(1,np.abs(np.int32(np.floor(points[j,0])))))\n",
        "        y = np.minimum(h,np.maximum(1,np.abs(np.int32(np.floor(points[j,1])))))\n",
        "        if x>w or y>h:\n",
        "            continue\n",
        "        x1 = x - np.int32(np.floor(f_sz/2))\n",
        "        y1 = y - np.int32(np.floor(f_sz/2))\n",
        "        x2 = x + np.int32(np.floor(f_sz/2))\n",
        "        y2 = y + np.int32(np.floor(f_sz/2))\n",
        "        dfx1 = 0\n",
        "        dfy1 = 0\n",
        "        dfx2 = 0\n",
        "        dfy2 = 0\n",
        "        change_H = False\n",
        "        if x1 < 1:\n",
        "            dfx1 = np.abs(x1)+1\n",
        "            x1 = 1\n",
        "            change_H = True\n",
        "        if y1 < 1:\n",
        "            dfy1 = np.abs(y1)+1\n",
        "            y1 = 1\n",
        "            change_H = True\n",
        "        if x2 > w:\n",
        "            dfx2 = x2 - w\n",
        "            x2 = w\n",
        "            change_H = True\n",
        "        if y2 > h:\n",
        "            dfy2 = y2 - h\n",
        "            y2 = h\n",
        "            change_H = True\n",
        "        x1h = 1+dfx1\n",
        "        y1h = 1+dfy1\n",
        "        x2h = f_sz - dfx2\n",
        "        y2h = f_sz - dfy2\n",
        "        if change_H:\n",
        "            H =  matlab_style_gauss2D((y2h-y1h+1,x2h-x1h+1),sigma)\n",
        "        im_density[y1-1:y2,x1-1:x2] = im_density[y1-1:y2,x1-1:x2] +  H;\n",
        "    return im_density\n",
        "     \n",
        "def matlab_style_gauss2D(shape=(3,3),sigma=0.5):\n",
        "    \"\"\"\n",
        "    2D gaussian mask - should give the same result as MATLAB's\n",
        "    fspecial('gaussian',[shape],[sigma])\n",
        "    \"\"\"\n",
        "    m,n = [(ss-1.)/2. for ss in shape]\n",
        "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
        "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
        "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
        "    sumh = h.sum()\n",
        "    if sumh != 0:\n",
        "        h /= sumh\n",
        "    return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f6568291b19046d881b95ce165559a77",
            "62dd9724f7e24952925b01cf6ffe5183",
            "115dbba4e0ad476994347f60d794ebbc",
            "bb463796e89046fc831f47d4c3ee875b",
            "7d43a92991b64abeadb64bb7de4ba3d4",
            "ddb92f3df50f465a88131cb0cf9356d9",
            "8a10935c258a47b99cb12024a7edb99b",
            "6e4bf150d0ae4a96b4ecb207a8c20c2a",
            "e83ea2aa788143efb953afc277f3d154",
            "05bc7808b10146f19c440142723623f4",
            "77ff7fb00ed5449e944745170cb79b26"
          ]
        },
        "id": "57xnB8PxJSCN",
        "outputId": "5a3fb9dc-c569-42cc-ba67-1c3c83684a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1045ae7d0e88>:355: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  split = np.int(len(image_datasets['train'])*0.2)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/104M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6568291b19046d881b95ce165559a77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 70.1457\n",
            "\n",
            "val:\n",
            "MAE:5.38, RMSE:5.38, MRE:0.0543\n",
            "test:\n",
            "MAE:32.17, RMSE:38.14, MRE:0.9540\n",
            "\n",
            "best MAE and MSE by val:  32.17 and 38.14 at Epoch 0\n",
            "best MAE and MSE by test: 32.17 and 38.14 at Epoch 0\n",
            "Epoch 1/4\n",
            "----------\n",
            "Train Loss: 52.9634\n",
            "\n",
            "val:\n",
            "MAE:34.50, RMSE:34.50, MRE:0.3485\n",
            "test:\n",
            "MAE:69.89, RMSE:72.18, MRE:1.6574\n",
            "\n",
            "best MAE and MSE by val:  32.17 and 38.14 at Epoch 0\n",
            "best MAE and MSE by test: 32.17 and 38.14 at Epoch 0\n",
            "Epoch 2/4\n",
            "----------\n",
            "Train Loss: 39.3105\n",
            "\n",
            "val:\n",
            "MAE:68.38, RMSE:68.38, MRE:0.6908\n",
            "test:\n",
            "MAE:106.11, RMSE:107.74, MRE:2.3753\n",
            "\n",
            "best MAE and MSE by val:  32.17 and 38.14 at Epoch 0\n",
            "best MAE and MSE by test: 32.17 and 38.14 at Epoch 0\n",
            "Epoch 3/4\n",
            "----------\n",
            "Train Loss: 28.9451\n",
            "\n",
            "val:\n",
            "MAE:54.72, RMSE:54.72, MRE:0.5527\n",
            "test:\n",
            "MAE:91.76, RMSE:93.80, MRE:2.0692\n",
            "\n",
            "best MAE and MSE by val:  32.17 and 38.14 at Epoch 0\n",
            "best MAE and MSE by test: 32.17 and 38.14 at Epoch 0\n",
            "Epoch 4/4\n",
            "----------\n",
            "Train Loss: 32.3184\n",
            "\n",
            "val:\n",
            "MAE:29.26, RMSE:29.26, MRE:0.2956\n",
            "test:\n",
            "MAE:66.73, RMSE:69.27, MRE:1.5439\n",
            "\n",
            "best MAE and MSE by val:  32.17 and 38.14 at Epoch 0\n",
            "best MAE and MSE by test: 32.17 and 38.14 at Epoch 0\n",
            "Training complete in 31m 53s\n",
            "test:\n",
            "MAE:32.17, RMSE:38.14, MRE:0.9540\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "==========================\n",
        "**Author**: Qian Wang, qian.wang173@hotmail.com\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import skimage.measure\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES=True\n",
        "import scipy\n",
        "import scipy.io\n",
        "import pdb\n",
        "plt.ion()   # interactive mode\n",
        "'''\n",
        "from model import CANNet\n",
        "from model_mcnn import MCNN\n",
        "from model_cffnet import CFFNet\n",
        "from model_csrnet import CSRNet\n",
        "from model_sanet import SANet\n",
        "from model_tednet import TEDNet\n",
        "from myInception_segLoss import headCount_inceptionv3\n",
        "from generate_density_map import generate_multi_density_map,generate_density_map\n",
        "'''\n",
        "\n",
        "IMG_EXTENSIONS = ['.JPG','.JPEG','.jpg', '.jpeg', '.PNG', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n",
        "def has_file_allowed_extension(filename, extensions):\n",
        "    \"\"\"Checks if a file is an allowed extension.\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the filename ends with a known image extension\n",
        "    \"\"\"\n",
        "    filename_lower = filename.lower()\n",
        "    return any(filename_lower.endswith(ext) for ext in extensions)\n",
        "\n",
        "def make_dataset(dir, extensions):\n",
        "    images = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    \"\"\"\n",
        "    for target in sorted(os.listdir(dir)):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "    \"\"\"\n",
        "    d = os.path.join(dir,'images')\n",
        "    for root, _, fnames in sorted(os.walk(d)):\n",
        "            for fname in sorted(fnames):\n",
        "                if has_file_allowed_extension(fname, extensions):\n",
        "                    image_path = os.path.join(root, fname)\n",
        "                    head,tail = os.path.split(root)\n",
        "                    label_path = os.path.join(head,'ground_truth','GT_'+fname[:-4]+'.mat')\n",
        "                    item = [image_path, label_path]\n",
        "                    images.append(item)\n",
        "\n",
        "    return images\n",
        "\n",
        "class ShanghaiTechDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None, phase='train',extensions=IMG_EXTENSIONS,patch_size=128,num_patches_per_image=4):\n",
        "        self.samples = make_dataset(data_dir,extensions)\n",
        "        self.image_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.phase = phase\n",
        "        self.patch_size = patch_size\n",
        "        self.numPatches = num_patches_per_image\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self,idx):        \n",
        "        img_file,label_file = self.samples[idx]\n",
        "        image = cv2.imread(img_file)\n",
        "        height, width, channel = image.shape\n",
        "        annPoints = scipy.io.loadmat(label_file)\n",
        "        annPoints = annPoints['image_info'][0][0][0][0][0]\n",
        "        positions = generate_density_map(shape=image.shape,points=annPoints,f_sz=15,sigma=4)\n",
        "        fbs = generate_density_map(shape=image.shape,points=annPoints,f_sz=25,sigma=1)\n",
        "        fbs = np.int32(fbs>0)\n",
        "        targetSize = [self.patch_size,self.patch_size]\n",
        "        height, width, channel = image.shape\n",
        "        if height < targetSize[0] or width < targetSize[1]:\n",
        "            image = cv2.resize(image,(np.maximum(targetSize[0]+2,height),np.maximum(targetSize[1]+2,width)))\n",
        "            count = positions.sum()\n",
        "            max_value = positions.max()\n",
        "            # down density map\n",
        "            positions = cv2.resize(positions, (np.maximum(targetSize[0]+2,height),np.maximum(targetSize[1]+2,width)))\n",
        "            count2 = positions.sum()\n",
        "            positions = np.minimum(positions*count/(count2+1e-8),max_value*10)\n",
        "            fbs = cv2.resize(fbs,(np.maximum(targetSize[0]+2,height),np.maximum(targetSize[1]+2,width)))\n",
        "            fbs = np.int32(fbs>0)\n",
        "        if len(image.shape)==2:\n",
        "            image = np.expand_dims(image,2)\n",
        "            image = np.concatenate((image,image,image),axis=2)\n",
        "        # transpose from h x w x channel to channel x h x w\n",
        "        image = image.transpose(2,0,1)\n",
        "        numPatches = self.numPatches\n",
        "        if self.phase == 'train':\n",
        "            patchSet, countSet, fbsSet = getRandomPatchesFromImage(image,positions,fbs,targetSize,numPatches)\n",
        "            x = np.zeros((patchSet.shape[0],3,targetSize[0],targetSize[1]))\n",
        "            if self.transform:\n",
        "              for i in range(patchSet.shape[0]):\n",
        "                #transpose to original:h x w x channel\n",
        "                x[i,:,:,:] = self.transform(np.uint8(patchSet[i,:,:,:]).transpose(1,2,0))\n",
        "            patchSet = x\n",
        "        if self.phase == 'val' or self.phase == 'test':\n",
        "            patchSet, countSet, fbsSet = getAllFromImage(image, positions, fbs)\n",
        "            patchSet[0,:,:,:] = self.transform(np.uint8(patchSet[0,:,:,:]).transpose(1,2,0))\n",
        "        return patchSet, countSet, fbsSet\n",
        "\n",
        "def getRandomPatchesFromImage(image,positions,fbs,target_size,numPatches):\n",
        "    # generate random cropped patches with pre-defined size, e.g., 224x224\n",
        "    imageShape = image.shape\n",
        "    if np.random.random()>0.5:\n",
        "        for channel in range(3):\n",
        "            image[channel,:,:] = np.fliplr(image[channel,:,:])\n",
        "        positions = np.fliplr(positions)\n",
        "        fbs = np.fliplr(fbs)\n",
        "    patchSet = np.zeros((numPatches,3,target_size[0],target_size[1]))\n",
        "    # generate density map\n",
        "    countSet = np.zeros((numPatches,1,target_size[0],target_size[1]))\n",
        "    fbsSet = np.zeros((numPatches,1,target_size[0],target_size[1]))\n",
        "    for i in range(numPatches):\n",
        "        topLeftX = np.random.randint(imageShape[1]-target_size[0]+1)#x-height\n",
        "        topLeftY = np.random.randint(imageShape[2]-target_size[1]+1)#y-width\n",
        "        thisPatch = image[:,topLeftX:topLeftX+target_size[0],topLeftY:topLeftY+target_size[1]]\n",
        "        patchSet[i,:,:,:] = thisPatch\n",
        "        # density map\n",
        "        position = positions[topLeftX:topLeftX+target_size[0],topLeftY:topLeftY+target_size[1]]\n",
        "        fb = fbs[topLeftX:topLeftX+target_size[0],topLeftY:topLeftY+target_size[1]]\n",
        "        position = position.reshape((1, position.shape[0], position.shape[1]))\n",
        "        fb = fb.reshape((1, fb.shape[0], fb.shape[1]))\n",
        "        countSet[i,:,:,:] = position\n",
        "        fbsSet[i,:,:,:] = fb\n",
        "    return patchSet, countSet, fbsSet\n",
        "\n",
        "def getAllPatchesFromImage(image,positions,target_size):\n",
        "    # generate all patches from an image for prediction\n",
        "    nchannel,height,width = image.shape\n",
        "    nRow = np.int(height/target_size[1])\n",
        "    nCol = np.int(width/target_size[0])\n",
        "    target_size[1] = np.int(height/nRow)\n",
        "    target_size[0] = np.int(width/nCol)\n",
        "    patchSet = np.zeros((nRow*nCol,3,target_size[1],target_size[0]))\n",
        "    for i in range(nRow):\n",
        "      for j in range(nCol):\n",
        "        patchSet[i*nCol+j,:,:,:] = image[:,i*target_size[1]:(i+1)*target_size[1], j*target_size[0]:(j+1)*target_size[0]]\n",
        "    return patchSet\n",
        "\n",
        "def getAllFromImage(image,positions,fbs):\n",
        "    nchannel, height, width = image.shape\n",
        "    patchSet =np.zeros((1,3,height, width))\n",
        "    patchSet[0,:,:,:] = image[:,:,:]\n",
        "    countSet = positions.reshape((1,1,positions.shape[0], positions.shape[1]))\n",
        "    fbsSet = fbs.reshape((1,1,fbs.shape[0], fbs.shape[1]))\n",
        "    return patchSet, countSet, fbsSet\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "class SubsetSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in range(len(self.indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=100, seg_loss=False, cl_loss=False, test_step=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_mae_val = 1e6\n",
        "    best_mae_by_val = 1e6\n",
        "    best_mae_by_test = 1e6\n",
        "    best_mse_by_val = 1e6\n",
        "    best_mse_by_test = 1e6\n",
        "    criterion1 = nn.MSELoss(reduce=False) # for density map loss\n",
        "    criterion2 = nn.BCELoss() # for segmentation map loss\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0        \n",
        "        # Iterate over data.\n",
        "        for index, (inputs, labels, fbs) in enumerate(dataloaders['train']):\n",
        "            labels = labels*100\n",
        "            labels = skimage.measure.block_reduce(labels.cpu().numpy(),(1,1,1,4,4),np.sum)\n",
        "            fbs = skimage.measure.block_reduce(fbs.cpu().numpy(),(1,1,1,4,4),np.max)\n",
        "            fbs = np.float32(fbs>0)\n",
        "            labels = torch.from_numpy(labels)\n",
        "            fbs = torch.from_numpy(fbs)\n",
        "            labels = labels.to(device)\n",
        "            fbs = fbs.to(device)\n",
        "            inputs = inputs.to(device)\n",
        "            inputs = inputs.view(-1,inputs.shape[2],inputs.shape[3],inputs.shape[4])\n",
        "            labels = labels.view(-1,labels.shape[3],labels.shape[4])\n",
        "            fbs = fbs.view(-1,fbs.shape[3],fbs.shape[4])\n",
        "            inputs = inputs.float()\n",
        "            labels = labels.float()\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(True):\n",
        "                output,fbs_out = model(inputs)\n",
        "                loss_den = criterion1(output, labels)\n",
        "                loss_seg = criterion2(fbs_out, fbs)\n",
        "                if cl_loss:\n",
        "                    th = 0.1*epoch+5 #cl2\n",
        "                else:\n",
        "                    th=1000 # no curriculum loss when th is set a big number\n",
        "                weights = th/(F.relu(labels-th)+th)\n",
        "                loss_den = loss_den*weights\n",
        "                loss_den = loss_den.sum()/weights.sum()\n",
        "                if seg_loss:\n",
        "                    loss = loss_den + 20*loss_seg\n",
        "                else:\n",
        "                    loss = loss_den\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "               \n",
        "        scheduler.step()    \n",
        "        epoch_loss = running_loss / dataset_sizes['train']            \n",
        "        \n",
        "        print('Train Loss: {:.4f}'.format(epoch_loss))\n",
        "        print()\n",
        "        if epoch%test_step==0:\n",
        "            tmp,epoch_mae,epoch_mse,epoch_mre=test_model(model,optimizer,'val')\n",
        "            tmp,epoch_mae_test,epoch_mse_test,epoch_mre_test = test_model(model,optimizer,'test')\n",
        "            if  epoch_mae < best_mae_val:\n",
        "                best_mae_val = epoch_mae\n",
        "                best_mae_by_val = epoch_mae_test\n",
        "                best_mse_by_val = epoch_mse_test\n",
        "                best_epoch_val = epoch\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if epoch_mae_test < best_mae_by_test:\n",
        "                best_mae_by_test = epoch_mae_test\n",
        "                best_mse_by_test = epoch_mse_test\n",
        "                best_epoch_test = epoch\n",
        "            print()\n",
        "            print('best MAE and MSE by val:  {:2.2f} and {:2.2f} at Epoch {}'.format(best_mae_by_val,best_mse_by_val, best_epoch_val))\n",
        "            print('best MAE and MSE by test: {:2.2f} and {:2.2f} at Epoch {}'.format(best_mae_by_test,best_mse_by_test, best_epoch_test))\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_model(model,optimizer,phase):\n",
        "    since = time.time()\n",
        "    model.eval()\n",
        "    mae = 0\n",
        "    mse = 0\n",
        "    mre = 0\n",
        "    pred = np.zeros((3000,2))\n",
        "    # Iterate over data.\n",
        "    for index, (inputs, labels, fbs) in enumerate(dataloaders[phase]):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        inputs = inputs.float()\n",
        "        labels = labels.float()\n",
        "        inputs = inputs.view(-1,inputs.shape[2],inputs.shape[3],inputs.shape[4])\n",
        "        labels = labels.view(-1,labels.shape[3],labels.shape[4])\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs,fbs_out = model(inputs)\n",
        "            outputs = outputs.to(torch.device(\"cpu\")).numpy()/100\n",
        "            pred_count = outputs.sum()\n",
        "        true_count = labels.to(torch.device(\"cpu\")).numpy().sum()\n",
        "        # backward + optimize only if in training phase\n",
        "        mse = mse + np.square(pred_count-true_count)\n",
        "        mae = mae + np.abs(pred_count-true_count)\n",
        "        mre = mre + np.abs(pred_count-true_count)/true_count\n",
        "        pred[index,0] = pred_count\n",
        "        pred[index,1] = true_count\n",
        "    pred = pred[0:index+1,:]\n",
        "    mse = np.sqrt(mse/(index+1))\n",
        "    mae = mae/(index+1)\n",
        "    mre = mre/(index+1)\n",
        "    print(phase+':')\n",
        "    print(f'MAE:{mae:2.2f}, RMSE:{mse:2.2f}, MRE:{mre:2.4f}')\n",
        "    time_elapsed = time.time() - since\n",
        "    return pred,mae,mse,mre\n",
        "\n",
        "#####################################################################\n",
        "# set parameters here\n",
        "seg_loss = True\n",
        "cl_loss = True\n",
        "test_step = 1\n",
        "batch_size = 6\n",
        "num_workers = 4\n",
        "patch_size = 128\n",
        "num_patches_per_image = 4\n",
        "data_dir = '/content/drive/MyDrive/testing/'\n",
        "\n",
        "# define data set\n",
        "image_datasets = {x: ShanghaiTechDataset(data_dir+x+'_data', \n",
        "                        phase=x, \n",
        "                        transform=data_transforms[x],\n",
        "                        patch_size=patch_size,\n",
        "                        num_patches_per_image=num_patches_per_image)\n",
        "                    for x in ['train','test']}\n",
        "image_datasets['val'] = ShanghaiTechDataset(data_dir+'train_data',\n",
        "                            phase='val',\n",
        "                            transform=data_transforms['val'],\n",
        "                            patch_size=patch_size,\n",
        "                            num_patches_per_image=num_patches_per_image)\n",
        "## split the data into train/validation/test subsets\n",
        "indices = list(range(len(image_datasets['train'])))\n",
        "split = np.int(len(image_datasets['train'])*0.2)\n",
        "\n",
        "val_idx = np.random.choice(indices, size=split, replace=False)\n",
        "train_idx = indices#list(set(indices)-set(val_idx))\n",
        "test_idx = range(len(image_datasets['test']))\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "test_sampler = SubsetSampler(test_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=image_datasets['train'],batch_size=batch_size,sampler=train_sampler, num_workers=num_workers)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=image_datasets['val'],batch_size=1,sampler=val_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=image_datasets['test'],batch_size=1,sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "dataset_sizes = {'train':len(train_idx),'val':len(val_idx),'test':len(image_datasets['test'])}\n",
        "dataloaders = {'train':train_loader,'val':val_loader,'test':test_loader}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "########################################################################\n",
        "# define models and training\n",
        "model = headCount_inceptionv3(pretrained=True)\n",
        "# model = MCNN()\n",
        "# model = SANet()\n",
        "# model = TEDNet(use_bn=True)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "\n",
        "model = train_model(model, optimizer, exp_lr_scheduler,\n",
        "                    num_epochs=5, #501\n",
        "                    seg_loss=seg_loss, \n",
        "                    cl_loss=cl_loss, \n",
        "                    test_step=test_step)\n",
        "                    \n",
        "pred,mae,mse,mre = test_model(model,optimizer,'test')\n",
        "scipy.io.savemat('./results.mat', mdict={'pred': pred, 'mse': mse, 'mae': mae,'mre': mre})\n",
        "model_dir = './'\n",
        "torch.save(model.state_dict(), model_dir+'saved_model.pt')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/FgSriVd88DqxH5oqs3ia",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6568291b19046d881b95ce165559a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62dd9724f7e24952925b01cf6ffe5183",
              "IPY_MODEL_115dbba4e0ad476994347f60d794ebbc",
              "IPY_MODEL_bb463796e89046fc831f47d4c3ee875b"
            ],
            "layout": "IPY_MODEL_7d43a92991b64abeadb64bb7de4ba3d4"
          }
        },
        "62dd9724f7e24952925b01cf6ffe5183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddb92f3df50f465a88131cb0cf9356d9",
            "placeholder": "",
            "style": "IPY_MODEL_8a10935c258a47b99cb12024a7edb99b",
            "value": "100%"
          }
        },
        "115dbba4e0ad476994347f60d794ebbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e4bf150d0ae4a96b4ecb207a8c20c2a",
            "max": 108857766,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e83ea2aa788143efb953afc277f3d154",
            "value": 108857766
          }
        },
        "bb463796e89046fc831f47d4c3ee875b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05bc7808b10146f19c440142723623f4",
            "placeholder": "",
            "style": "IPY_MODEL_77ff7fb00ed5449e944745170cb79b26",
            "value": " 104M/104M [00:01&lt;00:00, 132MB/s]"
          }
        },
        "7d43a92991b64abeadb64bb7de4ba3d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddb92f3df50f465a88131cb0cf9356d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a10935c258a47b99cb12024a7edb99b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e4bf150d0ae4a96b4ecb207a8c20c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e83ea2aa788143efb953afc277f3d154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05bc7808b10146f19c440142723623f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77ff7fb00ed5449e944745170cb79b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
